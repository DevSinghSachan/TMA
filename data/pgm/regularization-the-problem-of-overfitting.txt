
By now, you've seen a couple different
learning algorithms. Linear regression and
logistic regression. They work well for
many problems, but when you apply them to
certain machine learning applications,
they can run into a problem called over
fitting, that can cause them to perform
very poorly. What I'd like to do in this
video is explain to you what is this over
fitting problem and in the next few videos
after this we'll talk about a technique
called regularization that will allow us
to ameliorate or to reduce this over
fitting problem and get these learning
[inaudible] to maybe work much better. So
what is over fitting? Let's keep using our
running example of predicting housing
prices with linear regression, where we
want to predict the price as a function of
the size of the house. One thing we could
do is fit a linear function to this data.
And if we do that, maybe we get that sort
of straight line fit to the data. But this
isn't a very good model. Looking at the
data, it seems pretty clear that as the
size of the housing freezes, the, housing
prices plateau, or kind of flattens out as
we move to the right. And so, this,
algorithm doesn't fit the training set
very well. And we call this problem. Under
fitting and another term for this is that
this algorithm has high. Iii. Both of
these roughly mean that it's just not even
fitting the trading data very well. The
term bias is kind of historical or
technical one, but the idea is that If
we're fitting a straight line to the data,
then, it's as if the algorithm has a very
strong preconception or a very strong bias
that housing prices are going to vary
linearly with their size and despite the
data to the contrary, despite the evidence
to the contrary as preconceptions still
are biased, still causes it to fit a
straight line and just ends up being a
poor fit. Now, the middle. We could fit a
quadratic function to the data, and with
this data set. The [inaudible] function
maybe we get that kind of curve and that
works pretty well. And at the other
extreme would be if we were to fit say a
fourth polynomials in the data. So, here
we have five parameters data zero through
data four. And with that we can actually
build a curve that would process through
all five of our training examples. We
might get a curve that looks like this.
That, on the one hand, seems to do a very
good job fitting the training set. And
there's a process through all of my data,
at least. But this is sort [inaudible]
very wiggly curve, right? So I'm going up
and down all over the place. And we don't
actually think that's such a good model
for predicting housing prices. So this
problem we call over fitting. And another
term for this is that this algorithm has
high variance. The term high variance is
another sort of historical or technical
one, but the intuition is that, if we're
fitting such a high order polynomial, then
the hypothesis can fit, you know, it's
almost as if you can fit almost any
function. And the space of possible
hypotheses is just too large, or it's too
variable. And we don't have enough data to
constrain it to give us a good hypothesis.
So that's for all the setting. And in the
middle, there isn't really a name. But I'm
just gonna write, ya know just write.
Where second degree polynomial quadratic
function seems to be just right for
fitting this data. To recap a bit, the
problem of over fitting comes when if we
have two many features. That the very
hypothesis may fit the training set very
well. So, your cross function may be very
close to zero, maybe zero exactly. But you
may then end up with a curve like this
that you know tries too hard to fit the
training set so that it even fails to
generalize to new examples and that fails
to predict prices on new examples well.
And here the term generalize refers to how
well a hypothesis applies even to new
examples, that is to data to houses that
it hasn't seen in the training set. On
this slide we looked at over-fitting for
the case of linear regression. A similar
thing can apply to logistic regression as
well. Here is a logistic regression
example with two features X1 and X2. One
thing we could do is fit logistic
regression with just a simple hypothesis
like this. Where as usual G is my sigmoid
function. And if you do that you end up
with a hypothesis trying to use maybe just
a straight line to separate the positive
and the negative examples. And this
doesn't look like very good fit to the
hypothesis and so once again this is an
example of under-fitting or the hypothesis
having high bias. In contrast, if you were
to add to your features these quadratic
terms then you could get a position
boundary. That might look more like this.
And, you know, that's a pretty good fit to
the data. Probably, what, probably about
as good as we could get on this training
set. And finally, at the other extreme, if
you were to fit a very high order
polynomial, if you were to generate lots
of high order polynomial [inaudible]
features, then logistic regression may
contort itself, may try really hard to
find a [inaudible] boundary, that, Fits
your training data or go to great lengths
to contort itself to fit every single
training example well and you know if the
features x1 and x2 are for predicting
maybe the cancer to the cancer is
malignant benign beast tumors this
doesn't, this really doesn't look like a
very good hypothesis for making
predictions and so once again this is an
instance of over-fitting and of the
hypothesis having high variance and not
really, and being unlikely to generalize
well to new examples. Later in this course
when we talk about debugging and
diagnosing thinks that could go wrong with
learning algorithm we'll give you specific
tools to recognize when over fitting, and
also when under fitting may be occurring,
but for now lets talk about the problem of
if we think over fitting is occurring what
can we do to address it. In the previous
examples we had one or two-dimensional
data, so we could just plot the
hypotenuses and see what was going on and
select the appropriate degree polynomial,
so earlier. For the housing prices example
we can just plot the hypothesis. And you
know, we can see that it was fitting these
other [inaudible] function that goes all
the place for the closing prices. And we
could then use figures like these to
select an appropriate degree polynomial.
So, plotting the hypothesis could be one
way to try to decide what degree
polynomial to use but that doesn't always
work. And in fact, more often. We may have
learning problems, that where we just have
duller features and There, it's not just a
matter of selecting what the agreed
polynomial. And in fact, it, when we have
so many features, it also becomes much
harder to plot the data. And, it becomes
much harder to visualize it, to decide
what features to k-, to, to keep or not.
So, concretely, if we're trying to predict
housing prices, sometimes we can just have
a lot of different features. And all of
these features seem, you know. Maybe they
seem kinda useful. But, if we have a lot
of features and very little training data,
then over fitting can become a problem. In
order to address over fitting there are
two main options for things that we can
do. The first option is to try to reduce
the number of features. Concretely, one
thing we could do is manually look through
the list of features, and use that to try
to decide which are the more important
features? And therefore, which are the
features we should keep, and which are the
features we should throw out? Later in
this class, we'll also talk about model
selection algorithms, which are
algorithms. Automatically deciding which
features to keep and which features to
throw out. This idea of reducing the
number of features can work well and can
reduce auto feeding and when we talk about
model selection we'll go into this in much
greater depth. But the disadvantage is
that, by throwing away some of the
features, is also throwing away some of
the information you have about the
problem. For example, maybe all of those
features are actually useful for
protecting the price of a house. So maybe
we don't actually wanna throw some of our
information or throw some of our features
away. The second option. Which we'll talk
in, which we'll talk about in the next few
videos is regularization. Here we're going
to keep all the features. That we're going
to reduce the magnitude, or the values of
the parameters theta J. And this method
works well, we'll see, when we have a lot
of [inaudible], each of which contributes
a little bit to predicting the value of Y
like we had, like we saw in the housing
price protection example. Or we could have
a lot of [inaudible], each of which are,
you know, somewhat useful, so maybe we
don't want to throw them away. So. This
describes the idea of regularization at a
very high level. And, I realize that all
of these details probably don't make sense
to you yet. But in the next video, we'll
start to formulate exactly how, too apply
regularization, and exactly what
regularization means. And, then we'll
start to figure out how to use this to
make our learning algorithms work well and
avoid over fitting.
