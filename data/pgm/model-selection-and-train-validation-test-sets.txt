
Suppose you like to decide what degree of
probability or to fit to a data center or
what features to include to gives you a
learning algorithm. Or suppose you like to
chose de-regularization parameter founder
for learning algorithm. How do you do
that? These are called model selection
problems. And in our discussion of how to
do this, we'll talk about, not just how to
switch the data into a train and test set,
but how to switch the data into what we
discovered is called the train validation
and test sets. We'll see, in this video,
just what these things are, and how to use
them to do model selection. We've already
seen, a lot of time, the problem of over
fitting. And which, just because the
learning algorithm fits a training set
well, that doesn't mean there's a good
hypothesis. More generally, this is why
the training. Set zero is not a good
predictor for how well the hypothesis will
do on new examples. Concretely if you fit
some set of parameters data zero, data
one, data two, and so on to your training
set then the fact that. Your hypothesis
that was on the training set well this
doesn't mean much in terms of predicting
how well your hypothesis will generalize
to new examples not seen in training set
And a more general principle is that, once
your parameters were fit to some set of
data, maybe the training set, maybe
something else. Then the error of your
hypothesis as measured on that same data
set, such as the training error, that's
unlikely to be a good estimate of your
actual generalization error. That is, of
how well the hypothesis will generalize to
new examples. Now let's consider the model
selection problem. Let's say you try to
choose what degree of polynomial to fit to
data, so you could choose a linear
function, a quadratic function, a cubic
function, all the way up to a tenth order
polynomial. So as if there's one extra
parameter in this algorithm which I'm
going to denote D. Which is what degree of
polynomial do you want to pick. So is as
if, in addition to the theta parameters is
as if, there's one more parameter D that
you're trying to determine using your data
set. So the first option D=1, which is a
linear function, we can choose D=2, D=3,
all the way up to D=10. So like to fit
this extra so the parameter which I'm
denoting by D and concrete. Let's say that
you want to choose a model, that is chose
a degree of polynomial, choose one of
these ten models and fit that model and
also get some estimate of how well your
fitted hypothesis will generalize to new
examples. Here's one thing you could do,
you could first, take your first model and
minimize the training error and this would
give you some perimeter vector theta and
you could then take your second model, the
quadratic function and for that you.
[inaudible] And this will give you some
other parameter of [inaudible] theta. In
order to distinguish between these
different parameter vectors, I'm going to
use a superscript one, superscript two
there, where theta superscript one just
means the parameters I get by putting this
model to my training data, and theta
superscript two just means the parameters
I get by [inaudible] function to my
training data, and so on. And by fitting a
cubic model, I get parameters in three up
to, you know, say, theta ten. And one
thing you could do is then take. These
parameters and look at the test set errors
that can compute on my test set. J test of
one. J test of, theta two and so on. J
tests of the other three. And so on. So
lets take each of my hypothesis with the
corresponding parameters and just measure
their performance on the test set. Now one
thing I could do then is in order to
select one of these models I could then
see which model has the lowest test set
error. And let's just say for this example
that I ended up choosing the fifth order
polynomial. So this seems reasonable so
far. But now lets say I want to take my
[inaudible] hypothesis, this, this, fifth
order model, and lets say I want to ask,
how well does this model generalize. One
thing I could do is look at how well my
fifth order polynomial hypothesis had done
on my test set, but the problem is this
will not be a fair estimate of how well my
hypothesis generalizes. And the reason is,
what we've done is we fit this extra
parameter D that is this degree of
polynomial and we fit that parameter D
using the test set, namely we chose the
value of D that gave us the best possible.
Both performers on the test set. And so.
The performance of my parameter vector
theta five on the test set, that's likely
to be an overly optimistic estimate of
generalization error, right? So that
because I have fit this parameter D to my
test set, it is no longer fair to evaluate
my hypothesis on this test set. It's
because I fit my parameters to the test
set. I've chosen the degree, D of
polynomial using the test set. And so, my
hypothesis is likely to do better on this
test set than it would on new examples
that it hasn't seen before. And that's
which is, which is what I really care
about. So just to reiterate on the
previous slide we saw that if we fit some
set of [inaudible] you know theta zero,
theta one and so to some training set then
the performance of the fitted model on the
training set is not. Predictive of how
well. The hypothesis will generalize the
new examples, is because these parameters
were fit to the training set, so they're
likely to well on training set even if the
parameters don't do well on other
examples. And. In the procedure I just
described on this file, we just did the
same thing. And specifically, what we did
was we fit this parameter d to the test
set. And by having fit the parameter to
the test set, this means that the
performance of the hypothesis on that test
set may not be a fair estimate of how well
the hypothesis is, is likely to do on
examples we haven't seen before. To
address this problem in a model selection
setting, if we want to evaluate the
hypothesis, this is what we usually do
instead. Given a dataset instead of just
putting it into a train and test set, what
we're going to do is, instead, put it into
three g sets. And the first piece is going
to be called the training sets as usual.
So let me call this first part. The
training set and the second piece of this
data I'm going to call the cross
validation set. Cross validation and I'm
going to abbreviate cross validation as
CE. Sometimes it's also called a
validation set instead of cross validation
set. And then the last part I'm going to
call my usual test set. And a pretty,
pretty typical ratio at which to split
these things would be to send 60 percent
of your data to your training set, maybe
twenty percent to your cross validation
set, and twenty percent to your test set.
And these numbers can vary a little bit
but this sort of ratio will be pretty
typical. And so, our training sets will
now be only maybe 60 percent of the data,
and our cross-validation set, or our
validation set will have some number of
examples, I'm gonna denote that m
subscript cv, so that's the number of
cross-validation examples. Following our
earlier notational convention I'm going to
use x I c v comma y I c v to denote the
Ith cross-validation example. And finally.
We also have a test that's over here, with
a. And. Subscript test being the number of
test examples. So, now that we've defined
the training validation or cause
validation and test set, we can also
define the training error, cause
validation error, and test error. So
here's my training error. And I'm just
writing this as J subscript [inaudible] of
theta. This is pretty much the same thing,
it's usually the same thing as the J of
theta that we're writing so far. It's just
a training set error, you know, as
measured on your training set. And then J
subscript CV is my cause validation error.
It's pretty much what you'd expect. Just
[inaudible]. Training error accept
measured on the cross validation data set,
and here's my test set error, same as
before. So when faced with a multiple
selection problem like this, what we're
going to do is instead of using the test
set to select a model, we're instead going
to use the validation set or the cross
validation set to select a model.
Concretely, we're going to first take our
first hypothesis, take this first model,
and say minimize the cross function and
this will give me some [inaudible] theta.
For the linear model. And as before, I'm
gonna put a superscript one, just to
denote that this is the parameter for the
linear model. And we do the same thing for
the quadratic model [inaudible] parameter
vector theta two. [inaudible] parameter
vector theta three, and so on, down to,
say, the tenth or the polynomial. And what
I'm going to do is, instead of testing
these hypotheses on the test set, on this
day, I'm going to test them on the cause
validation set. I'm gonna measure J
subscript CV, to see how well each of
these hypotheses do. On my
cross-validation set. And then I'm going
to pick the hypothesis with the lowest
cross-validation error. So, for this
example, let's say, for the sake of
argument, that it was my fourth
[inaudible] polynomial that had the lowest
cause validation error. So in that case,
I'm going to pick this fourth [inaudible]
polynomial model. And finally. What this
means is that, that parameter d, remember
d was the degree of polynomial, right? So
d equals two, d equals three, up to d
equals ten. What we've done is we fit that
parameter d, we said d equals four, and we
did so using the cross-validation set. And
so this degree of polynomial, so the
parameter is no longer fit to the test
set. And so we've now saved away the test
set, and we can use the test set to
measure, or to estimate the generalization
error of the model that was selected. By
this algorithm. [sound] So that was
multiple selection and how you can take
your data, split it into a trained
validation and test set and use your cross
validation data to select [inaudible] and
evaluate it on the text set. One final
note, I should say that in the machine
learning after this practice today, there
are many people that will do that earlier
thing that I talked about and said that,
you know, isn't such a good idea of
selecting your model using the test set
and then using the same test set to report
the error as though selecting your degree.
Polynomial on the test set, and then
reporting the error on the test set as
though that were a good estimate of
generalization error. That sort of
practice, unfortunately many people do, do
it. If you have a massive, massive test
set it's maybe not a terrible thing to do.
But many practitioners most practitioners
of machine learning tend to advise against
that, and it's considered better practice
to have separate training, validation and
test sets. But I just warn you that
sometimes people do, you know, use the
same data for the purpose of the
validation set and for the purpose of test
sets. So you only have [inaudible]. He
said in the test that, let's consider
that's good practice, and you will see
some people do it, but if possible, I
would recommend against doing that myself.
