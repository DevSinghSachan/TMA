
We show the connection between
independences and the factorization of
the distribution in the context of
bayesian networks. Now we're going to
show that, that same kind of connection
holds also in the case of Markov
networks. So how are, so first of all we
need to come up with a similar notion of
what kind of independence's are encoded
by, by the structure of the graphical
model. So in this case we're going to have
a notion of separation. Which is the
analogous notion to deseparation except
that now there's no D for Directed it's
just a separation. And it actually is a
much simpler notion, because there's not
multiple kinds of different flows of
influence. You only have one type of edge,
the undirected edge, so it's very simple.
And, so we have the X and Y are separated
in H given Z if there's no active trail in
H and the notion of the active trail is
just, you know, if no node along the trail
is observed. So, so let's look at some
example separation properties. So for
example what does it take to separate A
from E? Well we can separate A from E in
several different ways. So we have the A
and E are separated. Given for example, B
and D Because B and D block both trails.
But also given just D, and also given B
and C, because B and C Again walk both
trails between A and E. So here's an
example of separation properties. Now we
can go ahead and prove an almost identical
theorem to the one that we proved in the
context of Bayesian networks. And it tells
us that if we have a separation property,
X and Y are separated in H given Z. And we
have distribution P that's factorizes over
H, then P satisfies The independent
statement x is independent of y given z.
And so just as in bayesian networks we can go
ahead and define the independence's that
are induced by the graph h as the ones
that are defined by the separation
property. And, just as in the context of
Bayesian Networks, we can go ahead and
define this as a term, we can define the
notion of an IMAP. And say that, in, and
say that H is an IMAP or independency map
of P if P satisfies all of the
independencies that we can read off the
graph structure of H. And the theorem that
we just proved, restated, says that if P
factorizes over H then H is an IMAP for P
because we have shown that if, P
factorizes over H then it satisfies all of
the independencies that one can read off
the graph H. Now in the context of
Bayesian Networks we also have the
converse theorem holding. The converse
also sort of holds in the context of
Markov Networks. The converse being that
independence, that if P satisfies the
independent statements associated with the
graph then it factorizes over the graph.
So, stated otherwise, if H is an IMAP for
P, that is P satisfies I of H, then P
factorizes over H. [inaudible] is that it
doesn't hold always, it only holds for a
positive distribution P, which means the
distribution P, which is strictly greater
than zero for all who's probability is
strictly greater than zero for all
assignments X. That is if you have a
distribution that involves a [inaudible]
relationships this property no longer
holds. So you almost have the converse
that you, you, we had in context of
[inaudible] networks but it requires one
additional and important assumption. So
once again we have two equivalent, almost
equivalent, views of graph structure.
Factorization, in which H allows P to be
represented. And, again the notion of an
IMAP, which is that I can read from H
independencies that hold in P. But once
again, if I tell you that a distribution P
factorizes over a graph, we can read from
the graph any independencies that must, we
can read from the graph a set of
independencies that must hold in P.
