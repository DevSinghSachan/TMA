
We define the belief propagation algorithm
with a message passing algorithm over a
fairly general data structure called a
cluster graph. And we show that if the
different cluster graph has certain
properties so does the belief propagation
algorithm. What we're going to do now, is
we're going to look at a special case of
the belief propagation algorithm, where we
pass messages not over a general cluster
graph, but rather over a data structure
called a clique tree, which as we'll show,
is a special case of a cluster graph. But,
interestingly, when we pass messages over a
clique tree, we can show that the belief
propagation algorithm has much stronger
performance guarantee than it does over
the general graph. And, specifically, we
can show that not only can we get
convergence much faster, we can also get
convergence to the exact answers for
probabilistic inference. So let's
consider the case of passing messages.
Tree and lets, I'll start with the
simplest kind of tree which is just a
chain. So in this case we have a chain
A-B-C-D-E and we're going to assume that
this is an undirected MRF or CRF and so we
have a, and for simplicity we're only
going to consider the pairwise potential
without loss of generality, and so we're
going to have just, pairwise potentials
here, that, for example, in the cluster AB
we're going to have the, the pairwise
belief over, the pairwise potential over
AB and so on. So, so here we have psi 1,
Psi 2. Psi 3. Psi 4. So now let's
think about the message passing process.
And we're going to start passing messages
from the left and so psi, so cluster the
message to cluster to and since there's no
incoming messages at this point, the
message that's get, that gets passed is
simply the sum over A of psi 1 of AB.
And that's a message whose scope is B. Now
we're going to have cluster two take that
message and pass the message to cluster
three. And so now it's taking its own
potential, psi 2, and multiplying
in this message into this. And the result
is a factor over B and C. And we're going
to sum out over B, because, we only want
to restrict to the scope C which is the
shared scope between cluster two and
cluster three. And so delta 2-3 is this
message over here. Now we're going to pass
messages one more time all the way to the
way down to the end. And so, now again we
are taking Delta 2-3, multiplying it with
psi 3 that comes from there. And that
gives us a message over scope D which here
gets sent to cluster four. You could also
pass messages in the other direction. So
this for example, would be what would
happen if we pass messages from cluster
four to cluster three, Regardless of when
that message gets passed the, there is no
other messages that cluster four would
multiply in when sending a message to
cluster three because, whether it got the
message from three or not, its not going
to multiply that in because remember you
only send the cluster message that doesn't
involve messages you received from it. And
so in this case the message that you get
is the, is the sum over E of the factor of
psi 4 which we get from here.
[sound], [sound] Passing messages in the
other direction again, we see that the
message cluster three passes to cluster
two which is delta 3-2, involved
psi 3 and this message over here.
And, finally exactly the same idea these
are the messages that gets passed. [sound]
The one important property that you have
when you run message passing in the way
that we just showed, is that somewhat
surprisingly relevant to what we've seen
before, the resulting beliefs that you get
are correct. And let's convince ourselves
of that, relative to cluster three and
so, in the order that we defined, cluster
one passed delta 1-2 to cluster two, which
was then used to define delta 23, at the
same time, we had delta 43 and so the
beliefs that we get here are the product
of psi 3 times delta 2-3, times delta 4-3. Let's unpack each
of those messages. So, delta 2-3 as
we remember, was defined here. And so
we're going to rewrite what delta 2-3 is as the summation over B, of
psi 2, delta 1-2. And similarly we unpack
the Delta 4-3 to get, using this
expression that we have over here. Right
there. Now we're going to do one further
level of unpacking and we are going to
remind ourselves of what delta 1-2
is, which is as defined over here and so
that gives us this expression over there.
And now look at what we have. We have a
product of all four initial factors in the
network; psi 1, psi 2, psi 3 and psi 4.
We have a summation over
all, the variables that are not in cluster
three, which are A, B and E. C and D are
in that cluster. So we sum out
over everything else. So already we see
that we have a product of factors.
Marginalized out. Well, we marginalized out
irrelevant variables or unnecessary
variables. Well this is really reminiscent
of variable elimination. And, if you
recall we talked about the correctness of
variable elimination. We said that
variable elimination is correct as long as
you, as when you sent out a variable you
first multiply in all factors that relate
to it. That involve the variable in their
scope. So lets convince ourselves that,
that is in fact what's going on here. So
lets look first at the variable E. And
when we sum out E, we're multiplying, we
only are, we're only involving psi 4,
but that's okay because only the cluster
four has E in its scope. Here we are
summing out A. And once again, only psi 1 involves, involves A in its scope.
Here we're summing out B. And really,
there should be parentheses here. They
were there implicitly before and so you
can see that when I'm summing out B of
involved psi 2, which involves B in its
scope. As well as what was left over from
summing out psi 1 which is the other factor
that B in its scope. And so can see that
I've marginalized out the variables in a
legal order, multiplying in only the
expression, multiplying in all of the
expressions that I needed to multiply
before I did this summation. And so this is a legal order of operations. And
therefore. It gives me a correct result at
the end. So, now lets expand that into a
somewhat broader into a somewhat broader
case. We're now going to define this notion
called, The Clique Tree. And, a clique
tree is an undirected tree, kind of like a
cluster graph was an undirected graph. So,
in this case, it's a tree. Such that, once
again, the nodes are clusters. And once
again we have edges between the clusters
which are called sepsets and in this case
where as before we had the sepset needed
to be a, only a subset of Ci intersection
Cj it could be smaller, it could be equal
or it could be smaller. Here we are going
to require that the sepset is actually the
intersection of the two adjacent
clusters. So now let's go and
understand what properties the cluster,
the clique tree that we showed has as a
special case of a cluster graph. So for a
cluster graph, we had the notion of family
preservation. And the reason that we had
family preservation is because if we have
a set of factors Phi, we needed to take
each phi k in Phi. And we needed to
assign it to some cluster so that, so that
each piece of information that we have is
represented somewhere. And the factor
needed to be such that the cluster could
accommodate its scope, so that the scope
of the factor was a subset of the set of
variables in a cluster. And the same
requirement holds here because
a clique tree is a special case of the cluster
graph. So that we have that for every
factor Phi K, there must exist at least one cluster whose scope is big enough to
accommodate that factor. The second
property that we had in cluster graphs was
the running intersection property. And
then, we clarified that the property that
we see here is the cluster graph variant.
We're going to make it. Specific to clique
trees in just a minute. So the cluster
graph variant told me before, that for
each pair of clusters, Ci and Cj, and a
variable X that is present in both of
them, there needed to exist a path between
Ci and Cj, for which all clusters and
sepsets contain X, and needs to be unique.
So what does that mean in the context of
clique trees? Let's assume that we have C7
containing X, and C6 containing X. And the
cluster, and the running intersection
property says that there needs to exist a
path between them, for which, all clusters
and sepsets contain X. Well, there's only
one path in a tree between C7 and C3, it's
this one. And so what that property now
tells me is that X has to be here, here,
here and here. Because the running,
otherwise the running intersection
property is going to be violated in this
case. So that means that in the context of
trees, the running intersection property
can actually be written much more simply.
It can, it can be read as saying that for
each pair of clusters Ci and Cj, and a
variable X in their intersection. Each
cluster and sepset in the unique path
between them must contain X. And so this
is the just the instantiation of running
intersection property to the context of
clique trees. So now, let's look at a, a
more complicated clique tree. This is the
clique tree, that rep-, corresponds to the
network that we previously used to
demonstrate variable elimination. This is
the elaboration of our student network.
And so this is, unlike the previous
example, this is actually a, a clique tree
for a Bayesian network, as opposed to a
Markov network. But as you'll see, there's
really no difference. So here we have set
of factors. In this case the factors are
CPD, and because of the family
preservation property we can be assured
that each factor, must be assignable to
some clique. And so here we have, P of C,
and P of D given C are assigned to this
clique, P of G given I D, P of I, and
P of S given I, and so on and so forth. So
each of the CPD's we have is assigned to
one and exactly one of the cliques in this
clique tree. And we can convince
ourselves, and we can convince ourselves
that this tree satisfies, for example, the
running intersection property. So let's
look, for example, at the variable G. The
variable G exists here, it exists here.
And sure enough, it exists here, here,
here, here, and here. And you can
similarly convince yourselves of that for
any of the other variable in this clique
tree, that the running intersection
property holds for that. Let's look at what
message passing looks like for this more
elaborate clique tree. So it's a little
bit only a slight bit more complicated. So
here we have, for example, delta. So it's
not even that much more complicated. So
here is delta 2-3, for example. And delta
2-3, which is the cluster between, which
is the message passed between cluster two and
cluster three. Takes psi 2, which is
the product of the factors assigned to
this clique. Actually, there's only
one in this case, so it's only P of G
given I and D. So that's psi 2.
Multiplies it with the incoming message,
delta 1-2, and sums out over D. The only
slight difference between this and the
previous example that we have is that here
for example, when cluster four passes the
message to cluster three, we can see that
over here there is two variables, J and L,
both of which must be summed out in order
to produce the message over the requisite
subsection. Now, we [inaudible], we
defined the running intersection property,
one of the really nice aspects of the
running intersection property is that it,
by itself together with family
preservation, suffices to prove the
correctness of message passing in a
clique tree. And the reason for that is
the following. If so let's consider a
variable X over here. And let's and what
we want to prove is that Is that if we
eliminate X at some point in the tree, at
some point in the message passing process,
then we have multiplied in all factors
that involve X. And, so here we have for
example X here and here. And by the
running intersection property, we have that
X is here and here as well, and on the
sepsets. And let's imagine that X is
eliminated on this edge. That is, the
message that's passed from C3 to C5
Doesn't, The message that passes thru C3
5o C5 doesn't involve X, so X must be summed
out at this point. Well at this point we
know that X cannot. Be here. Because if X
were in C-5, then it would also be in the
sepset. If, X in C-5 then X is also in
S-3-5, at which point, X wouldn't be
eliminated. And the reason you know that
if X is in C five, then X is in S three
five is because of the running
intersection property. So the running
intersection property relates the
structure of the graph to the structure,
to the time that those variables are
eliminated. So now let's convince
ourselves of the correctness in this
slightly more complicated example that,
that we've seen satisfies the running
intersection property. Here once again, we
have psi 1. Which is the product of
these two factors. Psi 2 which is this
one. Psi 3 which is these, the
product of these two factors. Psi 4.
Psi 5. And once again, let's for
example consider what we get at cluster
three. And you can see that we have
multiplied in all of the potential, psi 1,
psi 2, psi 3, psi 4
and psi 5. And that we have
eliminated all of the variable except for
GSI so here we had eliminated C, D, J, L
and H, leaving us only with the variables
G, S, I, which we want to keep. And we can
similarly convince ourselves, as we
did before, that when we eliminate a
variable, we've multiplied in all of the
factors that involve that variable. So to
summarize, we can take belief
propagation and run it over a cluster
graph that happens to be tree structured.
And, when we do that, it turns out that
the computation that we're running is
actually a variant of variable
elimination. And, because of that, the
resulting beliefs are guaranteed not only
to be calibrated but to actually be the
correct marginals of the un-normalized
density, P tilde Phi.
