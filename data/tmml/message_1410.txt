I've been working with Mallet the last few weeks, as a CS intern for a small company, to topic model roughly 68,000 .xml formatted documents. It seems that I must be somewhat familiar with the data to arbitrarily choose an appropriate number of topics, and for the case of ngrams and PAM, 1 or 2 or 3 word ngrams. Is this correct??
Secondly, it seems while topic-modeling the entire corpus, the "--use-ngram true" option overflows the java heap, although I can specify gram sizes and successfully run "import-dir" to produce the .mallet file.? PAM and standard LDA topic model the entire corpus without any heap overflow. Am I missing something?
???
Lastly, ngrams and PAM only output the output-state and output-doc-topics files. Omitting these options there are topic keys files, two xml files, and a few others all of which are very helpful in determining how and what topics Mallet has assigned. How can I determine Mallet's topic categorization without these extra files? It looks like I would have to go through the doc-topics file and take the first topic number for each document and bag these myself. I could write some code that reads in the 68,000+ lines and bags them, but it seems odd that mallet wouldn't do this, except for the LDA modeling. Am I going about this the wrong way? Thank you for you assistance.
Matt Kokidko
