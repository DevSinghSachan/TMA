Hi Mateusz,
Same here.
I had the same problem some month ago. Maybe I can help you with what I
figured out since then.
Depends on the implementation. You could estimate it or you could know it in
advance (e.g. you could know about your corpus, that topics are equally
frequent but there is only one per document).
Yes, yes. the matrix entry \beta_{i,j} is defined as $P(w_j|z^i=1)
Yes. And since they have to sum up to one, you have all parameters in fact.
Not sure what you think it has to do with N (the number of documents). The
dimensionality of \theta is k. Look at the picture at
http://en.wikipedia.org/wiki/Image:Dirichlet_distributions.png
The height of the surface is proportional to the probability you get that
point, when you sample \theta. And the distances to the cornerpoints represent
the probabilities you get for each of the (here three) topics (represented by
the corners).
Because \theta = { \theta_1, \theta_2, ..., \theta_n }, where each entry gives
the probability for one topic. And for topic z_n^i = 1 it is given by
\theta_i.
I don't know
It is given in the paper by Blei in the beginning of chapter three and in the
figure at http://en.wikipedia.org/wiki/Plate_notation
I am currently writing an overview in my thesis. Maybe this helps you:
Being a generative probabilistic model the basic assumption made in LDA is
that documents are generated by random processes. Each of these represents a
different topic z. A random process generates the words in the document by
sampling them from its own specific discrete probability distribution over the
words P(w| z ). A document can be created by one or more topics, each
having associated a distinct probability distribution over the words.
To represent the mixture of topics in a document, a multinomial distribution
\theta  is used.
For each word in the document the generating topic is selected by sampling
