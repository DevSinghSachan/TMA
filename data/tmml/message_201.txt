Sorry, I forgot to respond to the list...
Charles,
Our group has thought about similar issues. I should refer you to Walker & Ringger (2008) (KDD 2008 paper titled ?Model-Based Document Clustering with a Collapsed Gibbs Sampler?, to be presented next week ? should be available online soon, or we could provide you a copy), who investigated similar questions for a similar model (mixture of multinomials for document clustering).
I'll offer my thoughts and I hope others will correct or add as they see fit:
One complaint I?ve heard aimed at application of Gibbs sampling to LDA is that Gibbs sampling produces distributions (via samples), but typically only a single sample is used to produce a point estimate, and the distribution is ignored. In this case, some optimization procedure should be used, and variational EM fits this bill. That?s not to say the distributional information (multiple samples) can?t or shouldn?t be used, it just hasn?t for LDA (AFAIK; your other questions hint at why). At the same time, variational EM makes certain approximations (the mean-field approximation) which could also affect the results. I don?t believe anyone has compared them head-to-head for LDA, but the qualitative and quantitative results in different papers suggest they produce similar results. (For results on a different model, see Mark Johnson?s 2007 EMNLP paper for a comparison of the different approaches for HMMs on the unsupervised POS-tagging task).
In the LDA model, Gibbs produces samples from the posterior distribution of topics given words. The term ?converged? is perhaps misleading because Gibbs does not explicitly seek optima in the posterior distribution. Instead, as you probably know, we say that the sampler has ?converged in distribution?, meaning that the samples produced occur in proportion to their posterior density. A ?converged? Gibbs sampler will sample more frequently around the modes of the posterior distribution, which, by definition are maxima. However, sampling is typically a bad way to optimize; optima finding is an optimization task, which Gibbs is not meant to do. However, Gibbs IS (when properly converged) good at exploring, in the sense that it frequents the (possibly many) regions of high probability.
Unfortunately, when variables are highly correlated, Gibbs can "get stuck" and not actually sample the full distribution. In the paper by Walker and Ringger (KDD 2008), they touched on this for Gibbs sampling applied to a mixture of multinomials model. Although Gibbs outperforms garden variety EM (which only finds local maxima) on their  task, Gibbs on their model is likely NOT exploring the full space (i.e. confined to a local region that includes multiple maxima), and hence, is likely NOT sampling around the global optima at all (when it should be there most often). The same could be true for LDA, though I?m not aware of anyone who has analyzed this. However, it is unclear how much this ?stuckedness? hurts Gibbs.
If Gibbs has reached steady-state, then label-switching will have occurred and would have to be dealt with when considering multi-sample summaries. Variational methods do not have this same problem (since there are no samples).
If Gibbs has in fact reached steady-state, the marginal posterior distributions p(z_i|data) will be uniform in the limit. The Walker and Ringger results on a mixture of multinomials indicate that for relatively short chains (ranging from a few hundred to a few thousand samples), label switching almost never happens.
There are at least three quantities of interest: the posterior density (p(unknown variables|data), the marginal likelihood at the data (p(data) after marginalizing out all other variables), and the (log) likelihood of the data. If I?m not mistaken, the log likelihood of the data is Equation 2 of Griffiths and Steyvers? ?Finding Scientific Topics? paper. The same paper explains how to estimate the log marginal likelihood in the section on ?Model Selection?.
Depending on your purposes, the (log) posterior density may be useful (for instance, this can be plotted every iteration to help diagnose "convergence"). It is not possible to compute the log posterior density, but it is possible to compute a value proportional to the posterior density:
?_d???(?_d)??_k??(?_k ) ?
In case the equation does not show up properly, it is (in pseudo-latex):
\prod_d { \Beta( \alpha_d ) } \cdot \prod_k { \Beta( \beta_k ) }.
Where:
\Beta is the multinomial beta function (see http://en.wikipedia.org/wiki/Dirichlet_distribution)
d is the document index
k is the topic index, ranging from 1 to the number of topics
\alpha_d is a vector where the kth element is equal to the number of times topic k occurs in document d plus the prior alpha.
\beta_k is a vector where the vth element is equal to the number of times word type v has been assigned topic k plus the prior beta.
Mark Johnson's 2007 EMNLP paper on HMMs which compares EM, VEM, and Gibbs tries annealing methods but reports no significant difference in the results for Gibbs sampling, at least for his model.
Regards,
Robbie
