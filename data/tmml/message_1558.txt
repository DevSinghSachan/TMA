Hi Jonathan,
Thanks for your useful responses. Now, I have learned to predict the value of y_d. But  for the likelihood function D.24 which is used to Gibbs sampling, I still have three uncertain parts as follows:
1) In this function, the value of y_d is treated as an observed variance. In each iteration, if the value of y_d is set according to the known document-annotation distribution for the training data?
2) a and b are estimated using linear regression in the M-step, whether this linear regression is used for training data?
3) as you say, to do predictions we can just run the standard LDA inference scheme (with the topics learned at train time), and then regress on the expected value of z_bar. Is it possible to say that we can first learned the topics of the words, and then assign each topics with the learned y_d?
Thanks for taking your time to answer my questions. I really appreciate that.
Best regards,
wluo
At 2011-05-06 01:59:27?"Jonathan Chang" <foo at facebook.com> wrote:
Hi,
You are right that in prediction y_d is typically unobserved. But conveniently, y_d is a leaf node of the sLDA graphical model and hence can be dropped from the inference step. So to do predictions you can just run the standard LDA inference scheme (with the topics learned at train time), and then regress on the expected value of z_bar.
Cheers,
Jonathan
