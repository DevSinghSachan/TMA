Scott,
Have you looked at David Mimno's paper entitled "Organizing the OCA: Learning faceted subjects from a library of digital books?"  It is accessible at http://www.cs.umass.edu/~mimno/papers/f129-mimno.pdf.
His approach to sharing topics across collections is by first independently learning the topics within each collection followed by clustering the multinomial topics and estimating the parameters of dirichlet distributions from the topic clusters.  Each dirichlet is then used as a prior for topics and the topic models are re-run on each collection independently.  It works well, although is obviously sensitive to the clustering algorithm.  You can also use LDA or HDP within each collection.
Regarding the parallelizable topic model, I have been experimenting with online variational LDA (http://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf).  There is code available by one of the authors though I have reimplemented in Java.  It works very fast, and the expectation step for batches of training documents can be parallelized (I have implemented it with threads).  I have found that surprisingly, the predictive power can exceed the collapsed Gibbs sampling method.  The authors address this.  Though I have also found the interpretability of the topics to be somehow less useful.  One thing to keep in mind is that with this online method, the order of the documents matters in training.  I found best results if I randomly permuted the order of documents used to train the model.  I'm also working to implement the online variational HDP model (http://www.cs.princeton.edu/~blei/papers/WangPaisleyBlei2011.pdf).
Brad
