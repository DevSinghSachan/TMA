Dear all,
I have a small doubt related to the topic distribution learnt using LDA. I
run LDA on the bag-of-words representation of images and used 100 topics.
What I understand is that once the topic distribution is learnt, when we
pass a new document, we can infer the topic distribution for the new
document. This implies that the new document is mapped onto a point in the
topic simplex. So in the case of 100 topic used, the gamma parameter
inferred for one of the document is as given below
0.0114417486 0.0114417486 *22.4399196495* 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486 *
6.5112336259* 0.0114417486 0.0114417486 *3.6641598042
4.0760663892*0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486 *
14.9798257332* 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 *8.0275620380** 7.1683239595*
*4.2326164089*0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486
0.0114417486 0.0114417486 0.0114417486 *29.0032681288* 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
0.0114417486 0.0114417486 0.0114417486 0.0114417486 0.0114417486
The final alpha parameter is: 0.0114417484
This shows that only few topics are taking responsibility (highlighted) for
this document. However, my doubt is that, since the gamma-alpha is a
quantity that maps a document onto a probability simplex, is not it required
that the values add up to 1? This is not happening in my case.
Is there anything wrong in my training process that is giving such values?
However in the LDA paper itself it is mentioned that gamma-alpha is >1 for
few topics. So now what happens to the probability simplex?
regards
veena
