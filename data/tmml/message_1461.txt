Hi all,
Here is a tricky question. Let's say we are using symmetrical Dirichlet, as usual, as a prior for the topic-word distributions. We are setting the concentration parameter, we call it beta, of the Dirichlet to a small number depending on the number of unique words we have, let's say 0.001. So we thus favour the topics in which the probability is concentrated on a limited set of words.
Now, imagine we don't like the topics in which the probability is concentrated on 30 words, so we try to make the beta parameter even smaller... As we do this, the Dirichlet prior will begin to favour topics in which the probability is concentrated on a smaller set of words. As we decrease beta more, we eventually start to favour the topics which have most mass on one word, with the rest spread out through the other words in small amounts.
Using this approach, it is very hard to find a proper balance of the beta parameter so that we can find the topics which are not too sparse and at the same time not too dense. Ideally, I would like to set some prior on the beta such that topics which are concentrated more evenly on 3-4 words would be more probable than both, the topics that are concentrated on 1-2 words, and than the topics concentrated on > 4 words.
In short, I would like to be able to set such a prior on beta as to favour such topics that are good for both data exploration, and human interpretation. At the moment we have to play around with different betas and manually select the topics that are good... If there is an answer to that, such a prior on beta, that would be conjugate to Dirichlet, that would be even more than I ask for...
Any ideas, hints, paper references would be highly appreciated. Thank you!
Andrey
