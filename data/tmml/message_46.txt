Dear Dave and list --
thanks for the quick answer, the tree sampling procedure seems to work
with my model now. Still I am not quite sure, however, whether I am
doing the right thing _after_ the NCRP spits out a new branch (or loses
a branch because it ``lost'' all of its observations).
First: Am I right that NCRP sampling works on n(k), the number of times
any document in the corpus has been associated to topic k?
Maybe some more detail helps to understand my problem: When the NCRP
encounters a new branch, my bet is to sample any new topic multinomial
\vec\beta_{new} from the following distribution:
\vec\beta_{new} \sim H_m := p(\vec\beta_{new} | G_0, \vec w_m) =
Dir(\vec n_m + \eta)     (1)
where \vec n_m is the term frequency vector of the m'th document (that
generated the new topic/node). (This is an interpretation of Radford
Neal's DP sampling approach).
With this, all words in the current document can be re-associated to the
L topics in the document's branch of the tree. However, it is not clear
