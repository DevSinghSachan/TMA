Dave
I read your paper. Your method is interesting. But despite good experiment, the first method does not have theoretical guarantee, and the second method is a modification of original model. I think the difficulty lies on the choice of collapsing \phi and treating it as variable, so every updating on z distribution changes the expectation of \phi. So we can treat \phi as parameter rather than variable in the same way like variational LDA? By this way, every document will have its own Markov chain, rather than sharing one chain among the whole corpus. So parallel processing is trivial thing. We can use stochastic EM sampling scheme to solve the parameter, which is always necessary if you choose the optimize hyper-parameter like alpha or your model is more complicated than LDA. I see there will be a performance drop due to the needs of more iterations to reach a stable state (converged state?). But it should work.
