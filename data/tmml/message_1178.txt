Hi-
Question 1: Suppose a topic-model of a corpus is computed using K=K1. I'd
lie to verify the following intuition:  Given a corpus, if K is decreased
below K1, the resultant topics generated by LDA are 'merged' versions of the
topics from K=K1. For example, if 'business' and 'biology' were two separate
topics when K=K1, there might appear a single topic with words from
'business' and 'biology'. Similarly, when K is chosen to be greater than K1,
the resultant topics are 'sub-topics' of the topics from the K=K1 case. For
example, if 'business' was a topic in the K=K1 case, there may be a topic on
'banking' and another on ''entrepreneurship' in the case when K > K1. Is
this intuition correct? If so, is there a formal way to state this
intuition?
Question 2: I am trying to figure out whether the topics generated by LDA
can be meaningfully 'labelled' with a single word from the vocabulary of the
corpus. I have the following intuition for this:
Given that topic models are just based on word-co-occurence, it seems like a
meaningful label for each topic may be impossible to compute using the topic
model itself. How about using a topic-model of a second special corpus like
Wikipedia or a dictionary, to compute the unique word in the English
language to which the terms with high weight in a topic from the first
corpus are most related to. Any idea if something this has been tried? It
seems like a human can do this just b looking at the top words in a topic,
so it can't be too hard to compute.
Regards,
Suhas
